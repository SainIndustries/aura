---
phase: 08-agent-configuration-via-ansible
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ansible/playbooks/configure-agent.yml
  - src/ansible/requirements.yml
  - .github/workflows/provision-agent.yml
autonomous: true
user_setup:
  - service: tailscale
    why: "GitHub Actions runner needs to join Tailscale network to SSH to VM"
    env_vars:
      - name: TAILSCALE_OAUTH_CLIENT_ID
        source: "Already configured in Phase 7 (Tailscale admin console -> Settings -> OAuth clients)"
      - name: TAILSCALE_OAUTH_CLIENT_SECRET
        source: "Already configured in Phase 7"
  - service: hetzner
    why: "SSH key for Ansible to connect to VMs"
    env_vars:
      - name: HETZNER_SSH_PRIVATE_KEY
        source: "Private key corresponding to HETZNER_SSH_KEY_ID uploaded in Phase 7. Export from local SSH key or generate new pair and upload public key to Hetzner Cloud Console -> Security -> SSH Keys"

must_haves:
  truths:
    - "Ansible playbook targets remote hosts via SSH (not localhost)"
    - "Playbook waits for cloud-init completion before running configuration tasks"
    - "GitHub Actions runner joins Tailscale network before running Ansible"
    - "Ansible installs Docker, systemd services, and agent runtime on VM"
    - "Agent service starts automatically and is enabled for boot persistence"
    - "SSH key is securely injected and cleaned up after workflow completes"
  artifacts:
    - path: "src/ansible/playbooks/configure-agent.yml"
      provides: "Ansible playbook adapted for remote VM configuration"
      contains: "hosts: all"
    - path: "src/ansible/requirements.yml"
      provides: "Ansible Galaxy collection dependencies"
      contains: "community.general"
    - path: ".github/workflows/provision-agent.yml"
      provides: "Extended workflow with Tailscale runner setup and Ansible execution"
      contains: "ansible-playbook"
  key_links:
    - from: ".github/workflows/provision-agent.yml"
      to: "src/ansible/playbooks/configure-agent.yml"
      via: "ansible-playbook command in workflow step"
      pattern: "ansible-playbook.*configure-agent"
    - from: ".github/workflows/provision-agent.yml"
      to: "src/ansible/requirements.yml"
      via: "ansible-galaxy collection install"
      pattern: "ansible-galaxy.*requirements"
    - from: "src/ansible/playbooks/configure-agent.yml"
      to: "steps.provision.outputs.tailscale_ip"
      via: "dynamic inventory generated in workflow"
      pattern: "ansible_host.*tailscale_ip"
---

<objective>
Create an Ansible playbook adapted from openclaw-ansible for remote VM execution and integrate it into the GitHub Actions provisioning workflow. The playbook handles cloud-init timing, installs Docker and agent runtime, and starts the agent as a systemd service. The workflow is extended with Tailscale runner enrollment (for SSH access to VM via private network) and Ansible execution steps.

Purpose: This completes the provisioning pipeline by configuring freshly created VMs with the software stack needed to run OpenClaw agents. After Phase 7 creates the bare VM, Phase 8 configures it.

Output: Working Ansible playbook executed by GitHub Actions workflow that produces a fully configured, running agent on a remote VM.
</objective>

<execution_context>
@/Users/danielhuynh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/danielhuynh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-agent-configuration-via-ansible/08-RESEARCH.md
@.planning/phases/07-vm-provisioning-via-hetzner-api/07-02-SUMMARY.md
@.github/workflows/provision-agent.yml
@src/lib/provisioning/provision-vm.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Ansible playbook and Galaxy requirements for remote VM configuration</name>
  <files>
    src/ansible/playbooks/configure-agent.yml
    src/ansible/requirements.yml
  </files>
  <action>
Create the Ansible directory structure: `src/ansible/playbooks/` and `src/ansible/`.

**File 1: `src/ansible/requirements.yml`**
Ansible Galaxy collection dependencies:
```yaml
collections:
  - name: community.general
    version: ">=5.0.0"
```
Only `community.general` is needed (for `cloud_init_data_facts` module). Do NOT include `hetzner.hcloud` (not needed for static inventory).

**File 2: `src/ansible/playbooks/configure-agent.yml`**
Adapted from openclaw-ansible patterns for remote execution. Key design:

- `hosts: all` (NOT localhost) — targets dynamic inventory hosts via SSH
- `become: yes` — Hetzner VMs default to root user, become is safe no-op
- `gather_facts: no` initially — delay fact gathering until VM is confirmed ready

**Playbook stages (in order):**

1. **Wait for SSH connection** — `ansible.builtin.wait_for_connection` with delay: 10, timeout: 300, sleep: 5. This handles the window between Hetzner reporting server ready and SSH actually being available.

2. **Wait for cloud-init completion** — `community.general.cloud_init_data_facts` in a retry loop (retries: 60, delay: 10 = 10 min max). Check `cloud_init_data_facts.status.v1.stage == ''` to confirm cloud-init finished. Cloud-init from Phase 7 installs Tailscale and configures NTP — MUST complete before Ansible does anything else.

3. **Gather system facts** — `ansible.builtin.setup` after VM is confirmed ready.

4. **Verify Tailscale is running** — `ansible.builtin.command: tailscale status` (register result, `changed_when: false`). Fail with clear message if Tailscale not running (indicates cloud-init failure).

5. **Install Docker** — Use `ansible.builtin.apt` to install `docker.io` and `docker-compose-v2`. Include `update_cache: yes`, retries: 3, delay: 30 (handles transient apt lock from cloud-init). Then ensure Docker service is started and enabled via `ansible.builtin.systemd_service`.

6. **Install Node.js 20** — Add NodeSource repository and install `nodejs` package. Use the shell module to run NodeSource setup script: `curl -fsSL https://deb.nodesource.com/setup_20.x | bash -` with `args: creates: /usr/bin/node` for idempotency. Then `apt install -y nodejs`.

7. **Create openclaw system user** — `ansible.builtin.user` to create `openclaw` user (system: yes, shell: /usr/sbin/nologin, home: /opt/openclaw, create_home: yes). Add to docker group.

8. **Create agent configuration directory** — `ansible.builtin.file` to create `/opt/openclaw/config/` with owner openclaw.

9. **Create agent environment file** — `ansible.builtin.template` or `ansible.builtin.copy` to write `/opt/openclaw/config/agent.env` with agent_id, server_name, and API URL from inventory vars. Mode: 0600, owner: openclaw.

10. **Create systemd service file** — `ansible.builtin.copy` with `content:` to write `/etc/systemd/system/openclaw-agent.service`. Service should:
    - `Type=simple`
    - `User=openclaw`
    - `EnvironmentFile=/opt/openclaw/config/agent.env`
    - `ExecStart=/usr/bin/node /opt/openclaw/agent/index.js` (placeholder — actual agent binary path will be refined in later phases)
    - `Restart=always`
    - `RestartSec=10`
    - `StandardOutput=journal`
    - `StandardError=journal`
    - `SyslogIdentifier=openclaw-agent`
    - Install section: `WantedBy=multi-user.target`
    - Register: `notify: reload systemd`

11. **Reload systemd** — Handler triggered by service file change: `ansible.builtin.systemd: daemon_reload: yes`

12. **Enable and start agent service** — `ansible.builtin.systemd_service` with name: openclaw-agent, state: started, enabled: yes. Note: The agent binary won't exist yet (placeholder path), so use `failed_when: false` and register result. Log the status. The service will be properly functional when the actual agent code is deployed in a later phase.

13. **Security hardening** — Install and enable UFW (deny incoming, allow SSH on port 22, allow Tailscale). Install fail2ban with default config. Both via apt with retries.

14. **Final verification task** — Use `ansible.builtin.debug` to output a summary: Tailscale status, Docker version, Node.js version, UFW status, and whether the agent service unit exists.

**Important notes:**
- Do NOT set `connection: local` anywhere — use default SSH connection
- Use `ansible_python_interpreter: /usr/bin/python3` in inventory (not in playbook)
- All apt tasks should have `retries: 3, delay: 30, until: result is succeeded` to handle transient lock issues
- Use `ansible.builtin` fully qualified collection names throughout
  </action>
  <verify>
1. `cat src/ansible/playbooks/configure-agent.yml` exists and contains `hosts: all` (NOT localhost)
2. `cat src/ansible/requirements.yml` exists and contains `community.general`
3. Playbook does NOT contain `connection: local`
4. Playbook contains `wait_for_connection` task
5. Playbook contains `cloud_init_data_facts` task
6. Playbook contains Docker installation task
7. Playbook contains systemd service creation task
8. YAML syntax is valid: `python3 -c "import yaml; yaml.safe_load(open('src/ansible/playbooks/configure-agent.yml'))"` (or `npx yaml` if available)
  </verify>
  <done>
Ansible playbook exists at src/ansible/playbooks/configure-agent.yml targeting remote hosts via SSH with cloud-init readiness checks, Docker/Node.js installation, systemd service setup, and security hardening. Galaxy requirements file lists community.general collection.
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend GitHub Actions workflow with Tailscale runner and Ansible execution</name>
  <files>
    .github/workflows/provision-agent.yml
  </files>
  <action>
Modify the existing `.github/workflows/provision-agent.yml` to add Ansible configuration steps AFTER the VM provisioning step and BEFORE the success/failure callbacks.

**Changes to existing workflow:**

1. **Increase timeout** — Change `timeout-minutes: 15` to `timeout-minutes: 25`. Rationale: VM provisioning ~3min + cloud-init wait ~5min + Ansible playbook ~5-10min + buffer.

2. **Add Tailscale step** (after "Install dependencies", before "Send provisioning callback") — Use `tailscale/github-action@v3` to join the GitHub Actions runner to the Tailscale network. This is REQUIRED so the runner can SSH to the VM via its Tailscale IP.
```yaml
- name: Setup Tailscale on runner
  uses: tailscale/github-action@v3
  with:
    oauth-client-id: ${{ secrets.TAILSCALE_OAUTH_CLIENT_ID }}
    oauth-secret: ${{ secrets.TAILSCALE_OAUTH_CLIENT_SECRET }}
    tags: tag:ci
```
Note: Reuses existing TAILSCALE_OAUTH_CLIENT_ID and TAILSCALE_OAUTH_CLIENT_SECRET secrets from Phase 7. The `tags: tag:ci` tag must exist in Tailscale ACLs (or use empty tags if ACLs are permissive).

3. **Add Ansible installation step** (after "Provision VM" step) — Install Ansible on the runner:
```yaml
- name: Install Ansible
  run: |
    sudo apt-get update
    sudo apt-get install -y ansible
```
Note: Use apt instead of pipx — faster and sufficient for GitHub Actions runner (Ubuntu already has Python 3). apt provides Ansible core with community.general included in Ubuntu packages.

4. **Add Ansible Galaxy collections step** — Install required collections:
```yaml
- name: Install Ansible collections
  run: ansible-galaxy collection install -r src/ansible/requirements.yml
```

5. **Add SSH key setup step** — Write SSH private key to runner filesystem:
```yaml
- name: Configure SSH key for Ansible
  run: |
    mkdir -p ~/.ssh
    echo "${{ secrets.HETZNER_SSH_PRIVATE_KEY }}" > /tmp/ansible_ssh_key
    chmod 600 /tmp/ansible_ssh_key
```

6. **Add dynamic inventory generation step** — Generate inventory YAML from VM provisioning outputs:
```yaml
- name: Generate Ansible inventory
  run: |
    cat > /tmp/inventory.yml <<EOF
    all:
      hosts:
        agent_vm:
          ansible_host: ${{ steps.provision.outputs.tailscale_ip }}
          ansible_user: root
          ansible_ssh_private_key_file: /tmp/ansible_ssh_key
          ansible_python_interpreter: /usr/bin/python3
      vars:
        agent_id: "${{ inputs.agent_id }}"
        server_name: "${{ steps.provision.outputs.server_name }}"
        server_id: "${{ steps.provision.outputs.server_id }}"
    EOF
```

7. **Add Ansible playbook execution step** — Run the playbook with verbose output:
```yaml
- name: Configure VM with Ansible
  id: configure
  run: |
    ansible-playbook src/ansible/playbooks/configure-agent.yml \
      -i /tmp/inventory.yml \
      --ssh-common-args='-o StrictHostKeyChecking=accept-new' \
      -vv
```

8. **Add SSH key cleanup step** — Clean up sensitive material:
```yaml
- name: Cleanup SSH key
  if: always()
  run: |
    rm -f /tmp/ansible_ssh_key
    rm -f /tmp/inventory.yml
```

9. **Update success callback** — Change the status from "running" to "running" (no change needed to status, but the success callback now implicitly means VM provisioned AND configured). The failed_step in failure callback should include "configure" when Ansible fails. Update the failure callback to capture which step failed:
```yaml
- name: Send failure callback
  if: failure()
  run: |
    FAILED_STEP="unknown"
    if [ "${{ steps.provision.outcome }}" = "failure" ]; then
      FAILED_STEP="provision"
    elif [ "${{ steps.configure.outcome }}" = "failure" ]; then
      FAILED_STEP="configure"
    fi
    BODY=$(jq -n \
      --arg job_id "${{ inputs.job_id }}" \
      --arg status "failed" \
      --arg error "Provisioning workflow failed at $FAILED_STEP" \
      --arg failed_step "$FAILED_STEP" \
      '{job_id: $job_id, status: $status, error: $error, failed_step: $failed_step}')
    ...
```

**Step ordering in final workflow:**
1. Checkout
2. Setup Node.js
3. Install dependencies
4. Setup Tailscale on runner
5. Send provisioning callback
6. Heartbeat loop
7. Provision VM (existing)
8. Install Ansible
9. Install Ansible collections
10. Configure SSH key for Ansible
11. Generate Ansible inventory
12. Configure VM with Ansible
13. Stop heartbeat
14. Cleanup SSH key
15. Send success callback
16. Send failure callback

**Important:** The Tailscale step MUST come before "Provision VM" because `verifyEnrollment` in provision-vm.ts may need Tailscale connectivity. Actually, looking at the code more carefully, the Tailscale step needs to be early so the runner can reach the VM's Tailscale IP for Ansible. Place it after "Install dependencies" and before the provisioning callback.
  </action>
  <verify>
1. Workflow file is valid YAML: `python3 -c "import yaml; yaml.safe_load(open('.github/workflows/provision-agent.yml'))"`
2. `timeout-minutes` is 25 (increased from 15)
3. Workflow contains `tailscale/github-action` step
4. Workflow contains `ansible-playbook` command
5. Workflow contains `ansible-galaxy collection install` command
6. Workflow contains SSH key setup and cleanup steps
7. Workflow contains inventory generation step referencing `steps.provision.outputs.tailscale_ip`
8. Failure callback captures failed_step for both provision and configure outcomes
9. `npx tsc --noEmit` still passes (no TypeScript changes, but verify no regressions)
  </verify>
  <done>
GitHub Actions workflow extended with: Tailscale runner enrollment, Ansible installation, Galaxy collection installation, SSH key management, dynamic inventory generation from VM metadata, Ansible playbook execution targeting Tailscale IP, SSH key cleanup, and failure step attribution for both provisioning and configuration failures. Workflow timeout increased to 25 minutes.
  </done>
</task>

</tasks>

<verification>
1. **Playbook targets remote hosts:** `grep "hosts: all" src/ansible/playbooks/configure-agent.yml` returns match
2. **No localhost connection:** `grep -c "connection: local" src/ansible/playbooks/configure-agent.yml` returns 0
3. **Cloud-init wait exists:** `grep "cloud_init_data_facts" src/ansible/playbooks/configure-agent.yml` returns match
4. **Workflow has Ansible:** `grep "ansible-playbook" .github/workflows/provision-agent.yml` returns match
5. **Workflow has Tailscale:** `grep "tailscale/github-action" .github/workflows/provision-agent.yml` returns match
6. **SSH key cleanup:** `grep "Cleanup SSH key" .github/workflows/provision-agent.yml` returns match
7. **TypeScript still compiles:** `npx tsc --noEmit` passes
</verification>

<success_criteria>
- Ansible playbook at src/ansible/playbooks/configure-agent.yml targets `hosts: all` with SSH connection (not localhost)
- Playbook includes wait_for_connection and cloud_init_data_facts readiness checks
- Playbook installs Docker, Node.js, creates systemd service for agent
- Galaxy requirements file specifies community.general collection
- GitHub Actions workflow joins runner to Tailscale network
- Workflow generates dynamic inventory from VM provisioning outputs
- Workflow executes Ansible playbook against VM via Tailscale IP
- SSH key is written with chmod 600 and cleaned up after workflow
- Workflow timeout increased to accommodate Ansible execution
- Failure callback correctly attributes failures to provision or configure step
</success_criteria>

<output>
After completion, create `.planning/phases/08-agent-configuration-via-ansible/08-01-SUMMARY.md`
</output>
